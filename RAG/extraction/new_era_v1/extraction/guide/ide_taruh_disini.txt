	ocr_stats	line_stats	result
case_1	T	T	T
case_2	T	F	F
case_3	F	T	T
case_4	F	F	F

"ocr_status" dan "line_status"

jika "ocr_status" dan "line_status" True maka "decision_status" adalah True
jika "ocr_status" True dan "line_status" False maka "decision_status" adalah False
jika "ocr_status" False dan "line_status" True maka "decision_status" adalah True
jika "ocr_status" dan "line_status" False maka "decision_status" adalah False


jika "ocr_status" dan "line_status" dan "decision_status" adalah True maka halaman pdf tersebut akan saya manfaatkan multimodal dengan mengakses api LLM
jika "ocr_status" True dan "line_status" False dan "decision_status" False maka halaman tersebut akan saya ekstrak dengan OCR
jika "ocr_status" False dan "line_status" True dan "decision_status" True maka halaman tersebut akan saya manfaatkan multimodal dengan mengakses api LLM
jika "ocr_status" dan "line_status" False dan "decision_status" adalah False maka halaman pdf tersebut akan saya ekstrak dengan library ekstrak pdf biasa saja



text
tabel
chart
flowchart

---

saya memiliki script python untuk mengetahui penanganan apa yang diperlukan untuk mengekstrak pdf yang memiliki bermacam bentuk baik bisa langsung discan maupun ocr dan masih banyak lainnya

import os
import json
import PyPDF2
import pytesseract
from PIL import Image
import fitz  # PyMuPDF
import cv2
import numpy as np
from io import BytesIO

def detect_horizontal_lines(image, min_line_count=1, min_line_length_percent=20):
    height, width = image.shape[:2]
    min_line_length = int((min_line_length_percent / 100.0) * width)

    gray = cv2.cvtColor(image, cv2.COLORRGB2GRAY)
    , binary = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY_INV)

    horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (40, 1))
    detected_lines = cv2.morphologyEx(binary, cv2.MORPH_OPEN, horizontal_kernel, iterations=1)

    contours, _ = cv2.findContours(detected_lines, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    panjang_valid = [
        cv2.boundingRect(cnt)[2]
        for cnt in contours
        if cv2.boundingRect(cnt)[2] >= min_line_length
    ]
    return len(panjang_valid) >= min_line_count

def analyze_pdf(pdf_path, output_file="hasil_gabungan.json", min_text_length=50, min_line_count=1, min_line_length_percent=20):
    hasil_gabungan = {}

    with open(pdf_path, 'rb') as file:
        pdf_reader = PyPDF2.PdfReader(file)
        doc = fitz.open(pdf_path)
        total_pages = len(pdf_reader.pages)

        for i in range(total_pages):
            page_index = i + 1
            pdf_page = pdf_reader.pages[i]
            text = pdf_page.extract_text()

            # render image sekali saja pakai PyMuPDF
            page = doc[i]
            pix = page.get_pixmap(dpi=200)
            img = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.height, pix.width, pix.n)
            if pix.n == 4:
                img = cv2.cvtColor(img, cv2.COLOR_BGRA2RGB)
            else:
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

            # OCR
            if text and len(text.strip()) >= min_text_length:
                ocr_status = False
            else:
                pil_img = Image.fromarray(img)
                text_from_ocr = pytesseract.image_to_string(pil_img)
                if text_from_ocr and len(text_from_ocr.strip()) >= min_text_length:
                    ocr_status = True
                else:
                    ocr_status = "halaman kosong/gambar saja"

            # Line detection
            line_status = detect_horizontal_lines(img, min_line_count, min_line_length_percent)

            # Decision logic
            if isinstance(ocr_status, bool):
                ai_status = (ocr_status and line_status) or (not ocr_status and line_status)
            else:
                ai_status = False  # Default jika OCR gagal atau hasil ambigu

            hasil_gabungan[str(page_index)] = {
                "ocr_status": ocr_status,
                "line_status": line_status,
                "ai_status": ai_status
            }

            print(f"Halaman {page_index} diproses: OCR={ocr_status}, LINE={line_status}, AI={ai_status}")

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(hasil_gabungan, f, indent=4, ensure_ascii=False)

    print(f"Hasil gabungan disimpan di {output_file}")

# Contoh penggunaan
if name == "main":
    pdf_path = "ABF Indonesia Bond Index Fund.pdf"  # Ganti sesuai path file
    analyze_pdf(pdf_path, min_text_length=50, min_line_count=3, min_line_length_percent=10)

saat ini outputnya adalah file json yang memiliki informasi setiap halamannya seperti ini contohnya

{
    "1": {
        "ocr_status": false,
        "line_status": true,
        "ai_status": true
    },
    "2": {
        "ocr_status": false,
        "line_status": false,
        "ai_status": false
    },
    "3": {
        "ocr_status": false,
        "line_status": true,
        "ai_status": true
    },
}

dan seterusnya.

kemudian setelah didapatkan dokumen json yang memiliki informasi setiap halaman akan saya gunakan sebagai guide dalam melakukan ekstraksi.
rencannya proses ekstraksi akan saya buat dalam script berbeda agar tetap modular. bagaimana menurutmu?

adapun ekstraksinya akan menggunakan berbagai metode mengikuti skenario ini
jika "ocr_status" dan "line_status" dan "decision_status" adalah True maka halaman pdf tersebut akan saya manfaatkan multimodal dengan mengakses api LLM
jika "ocr_status" True dan "line_status" False dan "decision_status" False maka halaman tersebut akan saya ekstrak dengan OCR
jika "ocr_status" False dan "line_status" True dan "decision_status" True maka halaman tersebut akan saya manfaatkan multimodal dengan mengakses api LLM
jika "ocr_status" dan "line_status" False dan "decision_status" adalah False maka halaman pdf tersebut akan saya ekstrak dengan library ekstrak pdf biasa saja

bagaimana menurutmu?

sehingga saya rasa hasil ekstraksi akan saya simpan ke file json agar terstruktur. adapun ini rencana format file json hasil ekstraksinya

{
    "metadata": {
        "filename":"file_name",
        "total_pages": "value-total_pages",
        "extraction_date": "value-extraction_date",
        "processing_time": "value-processing_time"
    },
    "pages": {
        "1": {
            "analysis":{
                "ocr_status": false,
                "line_status": false,
                "ai_status": false
            },
            "extraction": {
                "method": "direct_extraction",
                "processing_time": "value-processing_time",
                "content_blocks":[
                    {
                        "block_id": 1,
                        "type": "text",
                        "content": "value-content"
                    }
                ]
            }
        },
        "2": {
            "analysis":{
                "ocr_status": true,
                "line_status": false,
                "ai_status": false
            },
            "extraction": {
                "method": "ocr",
                "processing_time": "value-processing_time",
                "content_blocks":[
                    {
                        "block_id": 1,
                        "type": "text",
                        "content": "value-content"
                    }
                ]
            }
        },
        "3": {
            "analysis":{
                "ocr_status": false,
                "line_status": true,
                "ai_status": true
            },
            "extraction": {
                "method": "multimodal_llm",
                "processing_time": "value-processing_time",
                "content_blocks":[
                    {
                        "block_id": 1,
                        "type": "text",
                        "content": "value-content"
                    },
                    {
                        "block_id": 2,
                        "type": "table",
                        "title": "value-title",
                        "data": [
                            {"header_1": "value-header_1-row_1", "header_2": "value-header_2-row_1"},
                            {"header_1": "value-header_1-row_2", "header_2": "value-header_2-row_2"},
                            {"header_1": "value-header_1-row_3", "header_2": "value-header_2-row_3"},
                            {"header_1": "value-header_1-row_4", "header_2": "value-header_2-row_4"},
                            {"header_1": "value-header_1-row_5", "header_2": "value-header_2-row_5"}
                        ],
                        "summary_table": "value-summary_table"
                    },
                    {
                        "block_id": 3,
                        "type": "text",
                        "content": "value-content"
                    },
                    {
                        "block_id": 4,
                        "type": "chart",
                        "chart_type": "line",
                        "title": "value-title",
                        "data": {
                            "labels": ["labels_1","labels_2","labels_3","labels_4","labels_5"],
                            "datasets": [
                                {
                                    "label": "label_1",
                                    "values": [6.8, 7.2, 5.9, 6.7, 7.5]
                                },
                                {
                                    "label": "label_2",
                                    "values": [6.2, 6.5, 5.4, 6.1, 6.9]
                                }
                            ]
                        },
                        "summary_chart": "value-summary_chart"
                    },
                    {
                        "block_id": 5,
                        "type": "text",
                        "content": "value-content"
                    },
                    {
                        "block_id": 6,
                        "type": "flowchart",
                        "title": "value-title",
                        "elements": [
                            {"type": "node", "id": "1", "text": "value-node_1", "connects_to": ["2"]},
                            {"type": "node", "id": "2", "text": "value-node_2", "connects_to": ["3"]},
                            {"type": "node", "id": "3", "text": "value-node_3", "connects_to": ["4"]},
                            {"type": "node", "id": "4", "text": "value-node_4", "connects_to": ["5"]},
                            {"type": "node", "id": "5", "text": "value-node_5", "connects_to": ["6"]},
                            {"type": "node", "id": "6", "text": "value-node_6", "connects_to": ["1"]}
                        ],
                        "summary_flowchart": "value-summary_flowchart"
                    },
                    {
                        "block_id": 7,
                        "type": "text",
                        "content": "value-content"
                    },
                    {
                        "block_id": 8,
                        "type": "image",
                        "description_image": "value-description_image"
                    }
                ]
            }
        },
        "4": {
            "analysis":{
                "ocr_status": false,
                "line_status": true,
                "ai_status": true
            },
            "extraction": {
                "method": "multimodal_llm",
                "processing_time": "value-processing_time",
                "content_blocks":[
                    {
                        "block_id": 1,
                        "type": "text",
                        "content": "value-content"
                    },
                    {
                        "block_id": 2,
                        "type": "table",
                        "title": "value-title",
                        "data": [
                            {"header_1": "value-header_1-row_1", "header_2": "value-header_2-row_1"},
                            {"header_1": "value-header_1-row_2", "header_2": "value-header_2-row_2"},
                            {"header_1": "value-header_1-row_3", "header_2": "value-header_2-row_3"},
                            {"header_1": "value-header_1-row_4", "header_2": "value-header_2-row_4"},
                            {"header_1": "value-header_1-row_5", "header_2": "value-header_2-row_5"}
                        ],
                        "summary_table": "value-summary_table"
                    },
                    {
                        "block_id": 3,
                        "type": "text",
                        "content": "value-content"
                    },
                    {
                        "block_id": 4,
                        "type": "chart",
                        "chart_type": "line",
                        "title": "value-title",
                        "data": {
                            "labels": ["labels_1","labels_2","labels_3","labels_4","labels_5"],
                            "datasets": [
                                {
                                    "label": "label_1",
                                    "values": [6.8, 7.2, 5.9, 6.7, 7.5]
                                },
                                {
                                    "label": "label_2",
                                    "values": [6.2, 6.5, 5.4, 6.1, 6.9]
                                }
                            ]
                        },
                        "summary_chart": "value-summary_chart"
                    },
                    {
                        "block_id": 5,
                        "type": "text",
                        "content": "value-content"
                    },
                    {
                        "block_id": 6,
                        "type": "flowchart",
                        "title": "value-title",
                        "elements": [
                            {"type": "node", "id": "1", "text": "value-node_1", "connects_to": ["2"]},
                            {"type": "node", "id": "2", "text": "value-node_2", "connects_to": ["3"]},
                            {"type": "node", "id": "3", "text": "value-node_3", "connects_to": ["4"]},
                            {"type": "node", "id": "4", "text": "value-node_4", "connects_to": ["5"]},
                            {"type": "node", "id": "5", "text": "value-node_5", "connects_to": ["6"]},
                            {"type": "node", "id": "6", "text": "value-node_6", "connects_to": ["1"]}
                        ],
                        "summary_flowchart": "value-summary_flowchart"
                    },
                    {
                        "block_id": 7,
                        "type": "text",
                        "content": "value-content"
                    },
                    {
                        "block_id": 8,
                        "type": "image",
                        "description_image": "value-description_image"
                    }
                ]
            }
        }
    }
}


bagaimana menurutmu? apakah ada hal yang saya lupakan? atau berikan masukan mu. jika ini sudah ok baru saya akan mulai melakukan pembuatan script untuk melakukan ekstraksi.
berikan saja responmu jangan inisiatif membuatkan program sebelum saya minta


---


saya memiliki sebuah proyek software untuk melakukan ekstraksi pdf yang hasilnya akan digunakan untuk basis pengetahuan RAG. mengingat akan beragamnya bentuk dan element pdf maka harus dibuat cara ekstraksi pdf yang baik, karena bisa saja berisikan teks, table, gambar, ocr dan lainnya.

saya memiliki scenario untuk melakukan identifikasi karakteristik pdf yang akan digunakan perhalaman
saya telah membuat script python untuk melakukan identifikasinya sehingga terbentuk sebuah file json yang berisikan informasi elemen apa yang ada di file pdf tersebut. berikut contohnya

{
    "1": {
        "ocr_status": false,
        "line_status": true,
        "ai_status": true
    },
    "2": {
        "ocr_status": false,
        "line_status": false,
        "ai_status": false
    },
    "3": {
        "ocr_status": false,
        "line_status": true,
        "ai_status": true
    }
}


script tersebut Bernama identifier.py dan hasilnya disimpan dalam file sample.json

kemudian dengan guide elemen yang ada disetiap halaman akan dilakukan ekstraksi berdasarkan tipe elementnya.
Adapun aturan ekstraksinya adalah sebagai berikut
1. jika "ocr_status" dan "line_status" dan "decision_status" adalah True maka halaman pdf tersebut akan saya manfaatkan multimodal dengan mengakses api LLM
2. jika "ocr_status" True dan "line_status" False dan "decision_status" False maka halaman tersebut akan saya ekstrak dengan OCR
3. jika "ocr_status" False dan "line_status" True dan "decision_status" True maka halaman tersebut akan saya manfaatkan multimodal dengan mengakses api LLM
4. jika "ocr_status" dan "line_status" False dan "decision_status" adalah False maka halaman pdf tersebut akan saya ekstrak dengan library ekstrak pdf biasa saja

untuk aturan 2 dan 4 saya sudah menyiapkan script terpisah masing-masing

aturan 2 saya gunakan script ocr_only.py dengan isi scriptnya sebagai berikut
"""
ocr_extractor.py - Module for OCR-based PDF text extraction
For pages where OCR is needed but no special formatting is detected
"""

import os
import json
import time
import datetime
import pytesseract
import fitz  # PyMuPDF
import PyPDF2
import numpy as np
import cv2
from PIL import Image
from pathlib import Path

def extract_with_ocr_method(pdf_path, page_num, existing_result=None, dpi=300):
    """
    Extract text from PDF using OCR for pages that need OCR processing
    but don't have complex formatting.
    
    Args:
        pdf_path (str): Path to the PDF file
        page_num (int): Page number to extract (1-based indexing)
        existing_result (dict, optional): Existing extraction result to update
        dpi (int): DPI resolution for rendering PDF to image
        
    Returns:
        dict: The extraction result for the specified page
    """
    start_time = time.time()
    
    # Create result structure if not provided
    if existing_result is None:
        result = {
            "analysis": {
                "ocr_status": True,
                "line_status": False,
                "ai_status": False
            },
            "extraction": {
                "method": "ocr",
                "processing_time": None,
                "content_blocks": []
            }
        }
    else:
        result = existing_result
        # Set extraction method and initialize content blocks if they don't exist
        result["extraction"] = {
            "method": "ocr",
            "processing_time": None,
            "content_blocks": []
        }
    
    try:
        # Convert PDF page to image using PyMuPDF
        doc = fitz.open(pdf_path)
        
        # Adjust for 0-based indexing
        pdf_page_index = page_num - 1
        
        if pdf_page_index < 0 or pdf_page_index >= len(doc):
            raise ValueError(f"Page {page_num} does not exist in PDF with {len(doc)} pages")
        
        page = doc[pdf_page_index]
        
        # Render page to image at specified DPI
        pix = page.get_pixmap(dpi=dpi)
        img = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.height, pix.width, pix.n)
        
        # Convert to RGB if needed
        if pix.n == 4:  # RGBA
            img = cv2.cvtColor(img, cv2.COLOR_RGBA2RGB)
        elif pix.n == 1:  # Grayscale
            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)
        
        # Convert to PIL image for Tesseract
        pil_img = Image.fromarray(img)
        
        # Perform OCR
        text = pytesseract.image_to_string(pil_img)
        
        # For OCR case, we only create one content block with all the text
        # regardless of paragraphs or line breaks
        if text and text.strip():
            result["extraction"]["content_blocks"] = [{
                "block_id": 1,
                "type": "text",
                "content": text.strip()
            }]
        else:
            result["extraction"]["content_blocks"] = [{
                "block_id": 1,
                "type": "text",
                "content": "No text content could be extracted via OCR from this page."
            }]
                
    except Exception as e:
        # Handle extraction errors
        result["extraction"]["content_blocks"] = [{
            "block_id": 1,
            "type": "text",
            "content": f"Error during OCR extraction: {str(e)}"
        }]
    
    # Calculate and record processing time
    processing_time = time.time() - start_time
    result["extraction"]["processing_time"] = f"{processing_time:.2f} seconds"
    
    return result

def process_pdf_pages(pdf_path, analysis_json_path, output_json_path, dpi=300):
    """
    Process all PDF pages that need OCR extraction based on analysis results
    
    Args:
        pdf_path (str): Path to the PDF file
        analysis_json_path (str): Path to the analysis JSON file
        output_json_path (str): Path to save the extraction results
        dpi (int): DPI resolution for rendering PDF to image
    """
    # Load analysis results
    with open(analysis_json_path, 'r', encoding='utf-8') as f:
        analysis_data = json.load(f)
    
    # Create or load output JSON
    if os.path.exists(output_json_path):
        with open(output_json_path, 'r', encoding='utf-8') as f:
            output_data = json.load(f)
    else:
        # Get PDF metadata
        with open(pdf_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            total_pages = len(pdf_reader.pages)
        
        # Create new output structure
        output_data = {
            "metadata": {
                "filename": Path(pdf_path).name,
                "total_pages": total_pages,
                "extraction_date": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "processing_time": "0 seconds"  # Will be updated later
            },
            "pages": {}
        }
        
        # Initialize pages structure with analysis data
        for page_num, page_analysis in analysis_data.items():
            output_data["pages"][page_num] = {
                "analysis": page_analysis
            }
    
    start_time = time.time()
    processed_count = 0
    
    # Process pages that need OCR extraction
    for page_num, page_data in analysis_data.items():
        # Check if this page needs OCR extraction
        if (page_data.get("ocr_status", False) and 
            not page_data.get("line_status", True) and 
            not page_data.get("ai_status", True)):
            
            # Check if this page has already been processed with OCR extraction
            if (page_num in output_data["pages"] and 
                "extraction" in output_data["pages"][page_num] and 
                output_data["pages"][page_num]["extraction"]["method"] == "ocr"):
                print(f"Page {page_num} already processed with OCR extraction. Skipping.")
                continue
            
            print(f"Processing page {page_num} with OCR extraction...")
            
            # Get existing result if available, otherwise create new
            existing_result = output_data["pages"].get(page_num, {"analysis": page_data})
            
            # Extract content
            result = extract_with_ocr_method(pdf_path, int(page_num), existing_result, dpi)
            
            # Update output data
            output_data["pages"][page_num] = result
            processed_count += 1
    
    # Update metadata
    total_processing_time = time.time() - start_time
    output_data["metadata"]["processing_time"] = f"{total_processing_time:.2f} seconds"
    
    # Save output data
    with open(output_json_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=4, ensure_ascii=False)
    
    print(f"OCR extraction completed. Processed {processed_count} pages.")
    return output_data

if __name__ == "__main__":
    # Example usage
    pdf_path = "ABF Indonesia Bond Index Fund.pdf"  # Replace with your PDF path
    analysis_json_path = "sample.json"  # Path to analysis JSON
    output_json_path = "hasil_ekstraksi.json"  # Path to save extraction results
    
    process_pdf_pages(pdf_path, analysis_json_path, output_json_path)



aturan 3 akan menggunakan script pdf_only.py sebagai berikut

"""
direct_extractor.py - Module for direct PDF text extraction
For pages where OCR isn't needed and no special formatting is detected
"""

import os
import json
import PyPDF2
import datetime
import time
from pathlib import Path

def extract_with_direct_method(pdf_path, page_num, existing_result=None):
    """
    Extract text directly from PDF using PyPDF2 for pages that don't need 
    special processing.
    
    Args:
        pdf_path (str): Path to the PDF file
        page_num (int): Page number to extract (1-based indexing)
        existing_result (dict, optional): Existing extraction result to update
        
    Returns:
        dict: The extraction result for the specified page
    """
    start_time = time.time()
    
    # Create result structure if not provided
    if existing_result is None:
        result = {
            "analysis": {
                "ocr_status": False,
                "line_status": False,
                "ai_status": False
            },
            "extraction": {
                "method": "direct_extraction",
                "processing_time": None,
                "content_blocks": []
            }
        }
    else:
        result = existing_result
        # Set extraction method and initialize content blocks
        result["extraction"] = {
            "method": "direct_extraction",
            "processing_time": None,
            "content_blocks": []
        }
    
    try:
        # Open PDF and extract text from the specific page
        with open(pdf_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            
            # Adjust for 0-based indexing in PyPDF2
            pdf_page_index = page_num - 1
            
            if pdf_page_index < 0 or pdf_page_index >= len(pdf_reader.pages):
                raise ValueError(f"Page {page_num} does not exist in PDF with {len(pdf_reader.pages)} pages")
            
            pdf_page = pdf_reader.pages[pdf_page_index]
            text = pdf_page.extract_text()
            
            # Create a single content block for the extracted text, regardless of paragraphs
            if text and text.strip():
                result["extraction"]["content_blocks"] = [{
                    "block_id": 1,
                    "type": "text",
                    "content": text.strip()
                }]
            else:
                result["extraction"]["content_blocks"] = [{
                    "block_id": 1,
                    "type": "text",
                    "content": "No text content could be extracted directly from this page."
                }]
                
    except Exception as e:
        # Handle extraction errors
        result["extraction"]["content_blocks"] = [{
            "block_id": 1,
            "type": "text",
            "content": f"Error during direct extraction: {str(e)}"
        }]
    
    # Calculate and record processing time
    processing_time = time.time() - start_time
    result["extraction"]["processing_time"] = f"{processing_time:.2f} seconds"
    
    return result

def process_pdf_pages(pdf_path, analysis_json_path, output_json_path):
    """
    Process all PDF pages that need direct extraction based on analysis results
    
    Args:
        pdf_path (str): Path to the PDF file
        analysis_json_path (str): Path to the analysis JSON file
        output_json_path (str): Path to save the extraction results
    """
    # Load analysis results
    with open(analysis_json_path, 'r', encoding='utf-8') as f:
        analysis_data = json.load(f)
    
    # Create or load output JSON
    if os.path.exists(output_json_path):
        with open(output_json_path, 'r', encoding='utf-8') as f:
            output_data = json.load(f)
    else:
        # Get PDF metadata
        with open(pdf_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            total_pages = len(pdf_reader.pages)
        
        # Create new output structure
        output_data = {
            "metadata": {
                "filename": Path(pdf_path).name,
                "total_pages": total_pages,
                "extraction_date": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "processing_time": "0 seconds"  # Will be updated later
            },
            "pages": {}
        }
        
        # Initialize pages structure with analysis data
        for page_num, page_analysis in analysis_data.items():
            output_data["pages"][page_num] = {
                "analysis": page_analysis
            }
    
    start_time = time.time()
    processed_count = 0
    
    # Process pages that need direct extraction
    for page_num, page_data in analysis_data.items():
        # Check if this page needs direct extraction
        if (not page_data.get("ocr_status", True) and 
            not page_data.get("line_status", True) and 
            not page_data.get("ai_status", True)):
            
            # Check if this page has already been processed with direct extraction
            if (page_num in output_data["pages"] and 
                "extraction" in output_data["pages"][page_num] and 
                output_data["pages"][page_num]["extraction"]["method"] == "direct_extraction"):
                print(f"Page {page_num} already processed with direct extraction. Skipping.")
                continue
            
            print(f"Processing page {page_num} with direct extraction...")
            
            # Get existing result if available, otherwise create new
            existing_result = output_data["pages"].get(page_num, {"analysis": page_data})
            
            # Extract content
            result = extract_with_direct_method(pdf_path, int(page_num), existing_result)
            
            # Update output data
            output_data["pages"][page_num] = result
            processed_count += 1
    
    # Update metadata
    total_processing_time = time.time() - start_time
    output_data["metadata"]["processing_time"] = f"{total_processing_time:.2f} seconds"
    
    # Save output data
    with open(output_json_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=4, ensure_ascii=False)
    
    print(f"Direct extraction completed. Processed {processed_count} pages.")
    return output_data

if __name__ == "__main__":
    # Example usage
    pdf_path = "ABF Indonesia Bond Index Fund.pdf"  # Replace with your PDF path
    analysis_json_path = "sample.json"  # Path to analysis JSON
    output_json_path = "hasil_ekstraksi.json"  # Path to save extraction results
    
    process_pdf_pages(pdf_path, analysis_json_path, output_json_path)


kedua sript tersebut akan menuliskan hasil ekstraksinya di file json Bernama hasil_ekstraksi.json dengan guide penyimpanan seperti ini

{
    "metadata": {
      "filename": "ABF Indonesia Bond Index Fund.pdf",
      "total_pages": 25,
      "extraction_date": "2025-04-14T10:30:00",
      "processing_time": "120.5s",
      "extraction_version": "1.0"
    },
    "pages": {
      "1": {
        "analysis": {
          "ocr_status": false,
          "line_status": false,
          "ai_status": false
        },
        "extraction": {
          "method": "direct_extraction",
          "library": "PyMuPDF",
          "processing_time": "0.3s",
          "content": {
            "text": "ABF INDONESIA BOND INDEX FUND\n\nLAPORAN KEUANGAN TRIWULAN I 2025\n\nDana investasi yang dikelola oleh PT XYZ Investment Management telah menunjukkan pertumbuhan yang konsisten dalam tiga bulan pertama tahun 2025. Laporan ini menyajikan kinerja keuangan dan alokasi aset untuk periode tersebut.",
            "tables": []
          },
          "content_blocks": [
            {
              "block_id": 1,
              "type": "text",
              "content": "ABF INDONESIA BOND INDEX FUND\n\nLAPORAN KEUANGAN TRIWULAN I 2025\n\nDana investasi yang dikelola oleh PT XYZ Investment Management telah menunjukkan pertumbuhan yang konsisten dalam tiga bulan pertama tahun 2025. Laporan ini menyajikan kinerja keuangan dan alokasi aset untuk periode tersebut."
            }
          ]
        }
      },
      "2": {
        "analysis": {
          "ocr_status": true,
          "line_status": false,
          "ai_status": false
        },
        "extraction": {
          "method": "ocr",
          "library": "pytesseract",
          "processing_time": "2.5s",
          "content": {
            "text": "RINGKASAN EKSEKUTIF\n\nSelama periode Januari - Maret 2025, ABF Indonesia Bond Index Fund mencatatkan imbal hasil sebesar 3.2%, lebih tinggi dari benchmark pasar yang hanya mencapai 2.8%. Kinerja ini didukung oleh strategi alokasi pada obligasi pemerintah tenor menengah yang memiliki imbal hasil optimal dengan risiko yang terukur.\n\nTotal dana kelolaan per 31 Maret 2025 mencapai Rp 1.25 triliun, meningkat 5% dari posisi akhir tahun 2024.",
            "tables": []
          },
          "content_blocks": [
            {
              "block_id": 1,
              "type": "text",
              "content": "RINGKASAN EKSEKUTIF\n\nSelama periode Januari - Maret 2025, ABF Indonesia Bond Index Fund mencatatkan imbal hasil sebesar 3.2%, lebih tinggi dari benchmark pasar yang hanya mencapai 2.8%. Kinerja ini didukung oleh strategi alokasi pada obligasi pemerintah tenor menengah yang memiliki imbal hasil optimal dengan risiko yang terukur.\n\nTotal dana kelolaan per 31 Maret 2025 mencapai Rp 1.25 triliun, meningkat 5% dari posisi akhir tahun 2024."
            }
          ]
        }
      },
      "3": {
        "analysis": {
          "ocr_status": false,
          "line_status": true,
          "ai_status": true
        },
        "extraction": {
          "method": "multimodal_llm",
          "model": "gpt-4-vision",
          "prompt_used": "Ekstrak semua konten dari gambar ini termasuk teks, tabel, diagram, dan grafik dengan mempertahankan urutan dan struktur aslinya...",
          "processing_time": "8.2s",
          "content": {
            "text": "ALOKASI ASET DAN KINERJA INVESTASI\n\nAlokasi aset ABF Indonesia Bond Index Fund saat ini didominasi oleh obligasi pemerintah dengan tenor 5-10 tahun, diikuti oleh obligasi korporasi rating AAA dan surat berharga negara jangka pendek.\n\nBerikut adalah rincian alokasi aset per 31 Maret 2025:\n\n[TABEL: Alokasi Aset per 31/03/2025]\n\nGrafik berikut menunjukkan tren kinerja investasi dalam 5 tahun terakhir dibandingkan dengan benchmark:\n\n[GRAFIK: Kinerja 5 Tahun vs Benchmark]\n\nProses investasi yang kami terapkan mengikuti alur sebagai berikut:\n\n[FLOWCHART: Proses Pengambilan Keputusan Investasi]\n\nDengan menerapkan proses yang ketat dan disiplin, kami berhasil mencapai kinerja yang konsisten melebihi benchmark selama 12 kuartal berturut-turut.",
            "tables": [
              {
                "table_id": 1,
                "title": "Alokasi Aset per 31/03/2025",
                "data": [
                  {"Jenis Aset": "Obligasi Pemerintah (5-10 tahun)", "Persentase": "45%"},
                  {"Jenis Aset": "Obligasi Pemerintah (1-5 tahun)", "Persentase": "25%"},
                  {"Jenis Aset": "Obligasi Korporasi (AAA)", "Persentase": "15%"},
                  {"Jenis Aset": "Obligasi Korporasi (AA)", "Persentase": "5%"},
                  {"Jenis Aset": "Surat Berharga Negara Jangka Pendek", "Persentase": "7%"},
                  {"Jenis Aset": "Kas & Setara Kas", "Persentase": "3%"}
                ],
                "text_representation": "Tabel: Alokasi Aset per 31/03/2025\n|-----------------------------------------|\n| Jenis Aset                        | Persentase |\n|-----------------------------------------|\n| Obligasi Pemerintah (5-10 tahun)  | 45%       |\n| Obligasi Pemerintah (1-5 tahun)   | 25%       |\n| Obligasi Korporasi (AAA)          | 15%       |\n| Obligasi Korporasi (AA)           | 5%        |\n| Surat Berharga Negara Jangka Pendek| 7%        |\n| Kas & Setara Kas                  | 3%        |\n|-----------------------------------------|\n"
              }
            ]
          },
          "content_blocks": [
            {
              "block_id": 1,
              "type": "text",
              "content": "ALOKASI ASET DAN KINERJA INVESTASI\n\nAlokasi aset ABF Indonesia Bond Index Fund saat ini didominasi oleh obligasi pemerintah dengan tenor 5-10 tahun, diikuti oleh obligasi korporasi rating AAA dan surat berharga negara jangka pendek.\n\nBerikut adalah rincian alokasi aset per 31 Maret 2025:",
              "position": {
                "top": 120,
                "left": 50,
                "width": 500,
                "height": 150
              }
            },
            {
              "block_id": 2,
              "type": "table",
              "title": "Alokasi Aset per 31/03/2025",
              "data": [
                {"Jenis Aset": "Obligasi Pemerintah (5-10 tahun)", "Persentase": "45%"},
                {"Jenis Aset": "Obligasi Pemerintah (1-5 tahun)", "Persentase": "25%"},
                {"Jenis Aset": "Obligasi Korporasi (AAA)", "Persentase": "15%"},
                {"Jenis Aset": "Obligasi Korporasi (AA)", "Persentase": "5%"},
                {"Jenis Aset": "Surat Berharga Negara Jangka Pendek", "Persentase": "7%"},
                {"Jenis Aset": "Kas & Setara Kas", "Persentase": "3%"}
              ],
              "text_representation": "Tabel: Alokasi Aset per 31/03/2025\n|-----------------------------------------|\n| Jenis Aset                        | Persentase |\n|-----------------------------------------|\n| Obligasi Pemerintah (5-10 tahun)  | 45%       |\n| Obligasi Pemerintah (1-5 tahun)   | 25%       |\n| Obligasi Korporasi (AAA)          | 15%       |\n| Obligasi Korporasi (AA)           | 5%        |\n| Surat Berharga Negara Jangka Pendek| 7%        |\n| Kas & Setara Kas                  | 3%        |\n|-----------------------------------------|\n",
              "position": {
                "top": 280,
                "left": 50,
                "width": 500,
                "height": 200
              }
            },
            {
              "block_id": 3,
              "type": "text",
              "content": "Grafik berikut menunjukkan tren kinerja investasi dalam 5 tahun terakhir dibandingkan dengan benchmark:",
              "position": {
                "top": 490,
                "left": 50,
                "width": 500,
                "height": 40
              }
            },
            {
              "block_id": 4,
              "type": "chart",
              "chart_type": "line",
              "title": "Kinerja 5 Tahun vs Benchmark",
              "data": {
                "labels": ["2020", "2021", "2022", "2023", "2024"],
                "datasets": [
                  {
                    "label": "ABF Indonesia Bond Index Fund",
                    "values": [6.8, 7.2, 5.9, 6.7, 7.5]
                  },
                  {
                    "label": "Benchmark",
                    "values": [6.2, 6.5, 5.4, 6.1, 6.9]
                  }
                ]
              },
              "description": "Grafik garis menunjukkan kinerja dana dibandingkan benchmark selama 5 tahun terakhir. Dana konsisten mengungguli benchmark setiap tahun.",
              "text_representation": "Grafik: Kinerja 5 Tahun vs Benchmark\nABF Indonesia Bond Index Fund: 2020 (6.8%), 2021 (7.2%), 2022 (5.9%), 2023 (6.7%), 2024 (7.5%)\nBenchmark: 2020 (6.2%), 2021 (6.5%), 2022 (5.4%), 2023 (6.1%), 2024 (6.9%)",
              "position": {
                "top": 540,
                "left": 50,
                "width": 500,
                "height": 300
              }
            },
            {
              "block_id": 5,
              "type": "text",
              "content": "Proses investasi yang kami terapkan mengikuti alur sebagai berikut:",
              "position": {
                "top": 850,
                "left": 50,
                "width": 500,
                "height": 40
              }
            },
            {
              "block_id": 6,
              "type": "flowchart",
              "title": "Proses Pengambilan Keputusan Investasi",
              "elements": [
                {"type": "node", "id": "1", "text": "Analisis Makroekonomi", "connects_to": ["2"]},
                {"type": "node", "id": "2", "text": "Evaluasi Kurva Yield", "connects_to": ["3"]},
                {"type": "node", "id": "3", "text": "Riset Kredit", "connects_to": ["4"]},
                {"type": "node", "id": "4", "text": "Konstruksi Portofolio", "connects_to": ["5"]},
                {"type": "node", "id": "5", "text": "Implementasi", "connects_to": ["6"]},
                {"type": "node", "id": "6", "text": "Monitoring & Evaluasi", "connects_to": ["1"]}
              ],
              "description": "Flowchart menunjukkan siklus proses pengambilan keputusan investasi yang dimulai dari analisis makroekonomi hingga monitoring dan evaluasi, yang kemudian kembali ke analisis makroekonomi.",
              "text_representation": "Flowchart: Proses Pengambilan Keputusan Investasi\nAnalisis Makroekonomi → Evaluasi Kurva Yield → Riset Kredit → Konstruksi Portofolio → Implementasi → Monitoring & Evaluasi → [kembali ke] Analisis Makroekonomi",
              "position": {
                "top": 900,
                "left": 50,
                "width": 500,
                "height": 250
              }
            },
            {
              "block_id": 7,
              "type": "text",
              "content": "Dengan menerapkan proses yang ketat dan disiplin, kami berhasil mencapai kinerja yang konsisten melebihi benchmark selama 12 kuartal berturut-turut.",
              "position": {
                "top": 1160,
                "left": 50,
                "width": 500,
                "height": 60
              }
            }
          ]
        }
      },
      "4": {
        "analysis": {
          "ocr_status": true,
          "line_status": true,
          "ai_status": true
        },
        "extraction": {
          "method": "multimodal_llm",
          "model": "gpt-4-vision",
          "prompt_used": "Ekstrak semua konten termasuk teks, tabel, diagram, gambar dan grafik dari gambar hasil scan ini...",
          "processing_time": "10.7s",
          "content": {
            "text": "ANALISIS RISIKO DAN PROSPEK INVESTASI\n\nPengelolaan risiko merupakan aspek fundamental dalam strategi investasi ABF Indonesia Bond Index Fund. Tim manajemen risiko secara aktif memantau berbagai parameter untuk memastikan profil risiko sesuai dengan mandate investasi.\n\n[TABEL: Profil Risiko per 31/03/2025]\n\nDalam mengelola risiko suku bunga, kami menerapkan strategi durasi yang dinamis seperti terlihat pada grafik di bawah ini:\n\n[GRAFIK: Strategi Durasi vs Pergerakan Suku Bunga]\n\nUntuk memahami sensitivitas portofolio terhadap perubahan suku bunga, berikut adalah analisis skenario yang telah kami lakukan:\n\n[FLOWCHART: Analisis Sensitivitas Portfolio]\n\nKantor kami yang baru di Distrik Bisnis Jakarta menjadi pusat operasional pengelolaan investasi dengan fasilitas riset terkini:\n\n[GAMBAR: Gedung Kantor ABF Indonesia]\n\nBerdasarkan analisis komprehensif terhadap indikator ekonomi terkini dan proyeksi kebijakan moneter Bank Indonesia, kami memperkirakan tren suku bunga akan cenderung stabil dalam dua kuartal ke depan sebelum berpotensi menurun di akhir tahun. Strategi kami adalah mempertahankan durasi moderat pada kuartal II-2025 dan secara bertahap memperpanjang durasi menjelang akhir tahun.",
            "tables": [
              {
                "table_id": 1,
                "title": "Profil Risiko per 31/03/2025",
                "data": [
                  {"Parameter Risiko": "Modified Duration", "Nilai": "5.2 tahun"},
                  {"Parameter Risiko": "Yield to Maturity", "Nilai": "6.75%"},
                  {"Parameter Risiko": "Credit Rating Rata-rata", "Nilai": "AA+"},
                  {"Parameter Risiko": "Value at Risk (95%, 1 bulan)", "Nilai": "1.8%"},
                  {"Parameter Risiko": "Tracking Error", "Nilai": "0.35%"},
                  {"Parameter Risiko": "Sharpe Ratio", "Nilai": "1.72"}
                ],
                "text_representation": "Tabel: Profil Risiko per 31/03/2025\n|------------------------------------|\n| Parameter Risiko          | Nilai       |\n|------------------------------------|\n| Modified Duration        | 5.2 tahun   |\n| Yield to Maturity        | 6.75%       |\n| Credit Rating Rata-rata  | AA+         |\n| Value at Risk (95%, 1 bulan) | 1.8%    |\n| Tracking Error           | 0.35%       |\n| Sharpe Ratio             | 1.72        |\n|------------------------------------|\n"
              }
            ]
          },
          "content_blocks": [
            {
              "block_id": 1,
              "type": "text",
              "content": "ANALISIS RISIKO DAN PROSPEK INVESTASI\n\nPengelolaan risiko merupakan aspek fundamental dalam strategi investasi ABF Indonesia Bond Index Fund. Tim manajemen risiko secara aktif memantau berbagai parameter untuk memastikan profil risiko sesuai dengan mandate investasi.",
              "position": {
                "top": 120,
                "left": 50,
                "width": 500,
                "height": 120
              }
            },
            {
              "block_id": 2,
              "type": "table",
              "title": "Profil Risiko per 31/03/2025",
              "data": [
                {"Parameter Risiko": "Modified Duration", "Nilai": "5.2 tahun"},
                {"Parameter Risiko": "Yield to Maturity", "Nilai": "6.75%"},
                {"Parameter Risiko": "Credit Rating Rata-rata", "Nilai": "AA+"},
                {"Parameter Risiko": "Value at Risk (95%, 1 bulan)", "Nilai": "1.8%"},
                {"Parameter Risiko": "Tracking Error", "Nilai": "0.35%"},
                {"Parameter Risiko": "Sharpe Ratio", "Nilai": "1.72"}
              ],
              "text_representation": "Tabel: Profil Risiko per 31/03/2025\n|------------------------------------|\n| Parameter Risiko          | Nilai       |\n|------------------------------------|\n| Modified Duration        | 5.2 tahun   |\n| Yield to Maturity        | 6.75%       |\n| Credit Rating Rata-rata  | AA+         |\n| Value at Risk (95%, 1 bulan) | 1.8%    |\n| Tracking Error           | 0.35%       |\n| Sharpe Ratio             | 1.72        |\n|------------------------------------|\n",
              "position": {
                "top": 250,
                "left": 50,
                "width": 500,
                "height": 200
              }
            },
            {
              "block_id": 3,
              "type": "text",
              "content": "Dalam mengelola risiko suku bunga, kami menerapkan strategi durasi yang dinamis seperti terlihat pada grafik di bawah ini:",
              "position": {
                "top": 460,
                "left": 50,
                "width": 500,
                "height": 50
              }
            },
            {
              "block_id": 4,
              "type": "chart",
              "chart_type": "line",
              "title": "Strategi Durasi vs Pergerakan Suku Bunga",
              "data": {
                "labels": ["Q1 2024", "Q2 2024", "Q3 2024", "Q4 2024", "Q1 2025"],
                "datasets": [
                  {
                    "label": "Portfolio Duration (tahun)",
                    "values": [4.8, 5.1, 5.5, 5.3, 5.2]
                  },
                  {
                    "label": "BI 7-Day Reverse Repo Rate (%)",
                    "values": [5.75, 5.75, 5.50, 5.25, 5.25]
                  }
                ]
              },
              "description": "Grafik garis menunjukkan perubahan durasi portofolio dibandingkan dengan pergerakan suku bunga BI selama 5 kuartal terakhir.",
              "text_representation": "Grafik: Strategi Durasi vs Pergerakan Suku Bunga\nPortfolio Duration (tahun): Q1 2024 (4.8), Q2 2024 (5.1), Q3 2024 (5.5), Q4 2024 (5.3), Q1 2025 (5.2)\nBI 7-Day Reverse Repo Rate (%): Q1 2024 (5.75), Q2 2024 (5.75), Q3 2024 (5.50), Q4 2024 (5.25), Q1 2025 (5.25)",
              "position": {
                "top": 520,
                "left": 50,
                "width": 500,
                "height": 250
              }
            },
            {
              "block_id": 5,
              "type": "text",
              "content": "Untuk memahami sensitivitas portofolio terhadap perubahan suku bunga, berikut adalah analisis skenario yang telah kami lakukan:",
              "position": {
                "top": 780,
                "left": 50,
                "width": 500,
                "height": 50
              }
            },
            {
              "block_id": 6,
              "type": "flowchart",
              "title": "Analisis Sensitivitas Portfolio",
              "elements": [
                {"type": "node", "id": "1", "text": "Kenaikan Suku Bunga +50bps", "connects_to": ["2"]},
                {"type": "node", "id": "2", "text": "Dampak NAB: -2.6%", "connects_to": []},
                {"type": "node", "id": "3", "text": "Kenaikan Suku Bunga +25bps", "connects_to": ["4"]},
                {"type": "node", "id": "4", "text": "Dampak NAB: -1.3%", "connects_to": []},
                {"type": "node", "id": "5", "text": "Tidak Ada Perubahan", "connects_to": ["6"]},
                {"type": "node", "id": "6", "text": "Dampak NAB: 0%", "connects_to": []},
                {"type": "node", "id": "7", "text": "Penurunan Suku Bunga -25bps", "connects_to": ["8"]},
                {"type": "node", "id": "8", "text": "Dampak NAB: +1.3%", "connects_to": []},
                {"type": "node", "id": "9", "text": "Penurunan Suku Bunga -50bps", "connects_to": ["10"]},
                {"type": "node", "id": "10", "text": "Dampak NAB: +2.6%", "connects_to": []}
              ],
              "description": "Flowchart menunjukkan berbagai skenario perubahan suku bunga dan dampaknya terhadap Nilai Aktiva Bersih (NAB) portofolio.",
              "text_representation": "Flowchart: Analisis Sensitivitas Portfolio\nKenaikan Suku Bunga +50bps → Dampak NAB: -2.6%\nKenaikan Suku Bunga +25bps → Dampak NAB: -1.3%\nTidak Ada Perubahan → Dampak NAB: 0%\nPenurunan Suku Bunga -25bps → Dampak NAB: +1.3%\nPenurunan Suku Bunga -50bps → Dampak NAB: +2.6%",
              "position": {
                "top": 840,
                "left": 50,
                "width": 500,
                "height": 200
              }
            },
            {
              "block_id": 7,
              "type": "text",
              "content": "Kantor kami yang baru di Distrik Bisnis Jakarta menjadi pusat operasional pengelolaan investasi dengan fasilitas riset terkini:",
              "position": {
                "top": 1050,
                "left": 50,
                "width": 500,
                "height": 50
              }
            },
            {
              "block_id": 8,
              "type": "image",
              "title": "Gedung Kantor ABF Indonesia",
              "description": "Foto gedung kantor ABF Indonesia di Distrik Bisnis Jakarta dengan arsitektur modern dan lingkungan hijau.",
              "caption": "Kantor Pusat ABF Indonesia di Jakarta",
              "text_representation": "[GAMBAR: Gedung Kantor ABF Indonesia - Kantor Pusat ABF Indonesia di Jakarta]",
              "position": {
                "top": 1110,
                "left": 50,
                "width": 500,
                "height": 300
              }
            },
            {
              "block_id": 9,
              "type": "text",
              "content": "Berdasarkan analisis komprehensif terhadap indikator ekonomi terkini dan proyeksi kebijakan moneter Bank Indonesia, kami memperkirakan tren suku bunga akan cenderung stabil dalam dua kuartal ke depan sebelum berpotensi menurun di akhir tahun. Strategi kami adalah mempertahankan durasi moderat pada kuartal II-2025 dan secara bertahap memperpanjang durasi menjelang akhir tahun.",
              "position": {
                "top": 1420,
                "left": 50,
                "width": 500,
                "height": 120
              }
            }
          ]
        }
      }
    },
    "summary": {
      "extraction_methods": {
        "direct_extraction": 10,
        "ocr": 5,
        "multimodal_llm": 10
      },
      "content_stats": {
        "total_pages": 25,
        "total_text_blocks": 120,
        "total_tables": 15,
        "total_charts": 8,
        "total_flowcharts": 5,
        "total_images": 7,
        "avg_text_length_per_page": 2500
      },
      "processing_stats": {
        "total_processing_time": "120.5s",
        "avg_processing_time_per_page": "4.82s",
        "processing_time_by_method": {
          "direct_extraction": "5.3s",
          "ocr": "12.5s",
          "multimodal_llm": "102.7s"
        }
      }
    }
  }

sekarang saya ingin meminta bantuan mu
karena saya telah membuat script :
1. identifier.py
2. ocr_only.py
3. pdf_only.py

apakah kamu bisa membuat untuk ekstraksi dengan penanganan multimodal apinya? jika bisa saya telah memiliki script yang saya harap bisa jadi guide mu dalam membuat multimodal api ekstraktornya


import PIL.Image
import google.generativeai as genai
from google.generativeai import types
import os
from dotenv import load_dotenv

load_dotenv()

GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')

image_path_1 = "image/7_Tabel_N_Halaman_Normal_V1_page-0001.jpg"
image_path_2 = "image/7_Tabel_N_Halaman_Normal_V1_page-0002.jpg"

pil_image_1 = PIL.Image.open(image_path_1)
pil_image_2 = PIL.Image.open(image_path_2)

genai.configure(api_key=GOOGLE_API_KEY) #configure api key

model = genai.GenerativeModel('gemini-2.0-flash') #gunakan model yang sesuai

response = model.generate_content(
    [
        "apakah kamu bisa melakukan ekstraksi tabel dari file pdf yang saya berikan ini? serta outputnya kalau bisa dalam bentuk json",
        pil_image_1, pil_image_2
    ]
)

print(response.text)




---

saya memiliki sebuah proyek software untuk melakukan ekstraksi pdf yang hasilnya akan digunakan untuk basis pengetahuan RAG. mengingat akan beragamnya bentuk dan element pdf maka harus dibuat cara ekstraksi pdf yang baik, karena bisa saja berisikan teks, table, gambar, ocr dan lainnya.

saya memiliki scenario untuk melakukan identifikasi karakteristik pdf yang akan digunakan perhalaman
saya telah membuat script python untuk melakukan identifikasinya sehingga terbentuk sebuah file json yang berisikan informasi elemen apa yang ada di file pdf tersebut. berikut contohnya

{
    "1": {
        "ocr_status": false,
        "line_status": true,
        "ai_status": true
    },
    "2": {
        "ocr_status": false,
        "line_status": false,
        "ai_status": false
    },
    "3": {
        "ocr_status": false,
        "line_status": true,
        "ai_status": true
    }
}


script tersebut Bernama identifier.py dan hasilnya disimpan dalam file sample.json

kemudian dengan guide elemen yang ada disetiap halaman akan dilakukan ekstraksi berdasarkan tipe elementnya.
Adapun aturan ekstraksinya adalah sebagai berikut
1. jika "ocr_status" dan "line_status" dan "decision_status" adalah True maka halaman pdf tersebut akan saya manfaatkan multimodal dengan mengakses api LLM
2. jika "ocr_status" True dan "line_status" False dan "decision_status" False maka halaman tersebut akan saya ekstrak dengan OCR
3. jika "ocr_status" False dan "line_status" True dan "decision_status" True maka halaman tersebut akan saya manfaatkan multimodal dengan mengakses api LLM
4. jika "ocr_status" dan "line_status" False dan "decision_status" adalah False maka halaman pdf tersebut akan saya ekstrak dengan library ekstrak pdf biasa saja

untuk aturan 2 dan 4 saya sudah menyiapkan script terpisah masing-masing

aturan 2 saya gunakan script ocr_only.py dengan isi scriptnya sebagai berikut
"""
ocr_extractor.py - Module for OCR-based PDF text extraction
For pages where OCR is needed but no special formatting is detected
"""

import os
import json
import time
import datetime
import pytesseract
import fitz  # PyMuPDF
import PyPDF2
import numpy as np
import cv2
from PIL import Image
from pathlib import Path

def extract_with_ocr_method(pdf_path, page_num, existing_result=None, dpi=300):
    """
    Extract text from PDF using OCR for pages that need OCR processing
    but don't have complex formatting.
    
    Args:
        pdf_path (str): Path to the PDF file
        page_num (int): Page number to extract (1-based indexing)
        existing_result (dict, optional): Existing extraction result to update
        dpi (int): DPI resolution for rendering PDF to image
        
    Returns:
        dict: The extraction result for the specified page
    """
    start_time = time.time()
    
    # Create result structure if not provided
    if existing_result is None:
        result = {
            "analysis": {
                "ocr_status": True,
                "line_status": False,
                "ai_status": False
            },
            "extraction": {
                "method": "ocr",
                "processing_time": None,
                "content_blocks": []
            }
        }
    else:
        result = existing_result
        # Set extraction method and initialize content blocks if they don't exist
        result["extraction"] = {
            "method": "ocr",
            "processing_time": None,
            "content_blocks": []
        }
    
    try:
        # Convert PDF page to image using PyMuPDF
        doc = fitz.open(pdf_path)
        
        # Adjust for 0-based indexing
        pdf_page_index = page_num - 1
        
        if pdf_page_index < 0 or pdf_page_index >= len(doc):
            raise ValueError(f"Page {page_num} does not exist in PDF with {len(doc)} pages")
        
        page = doc[pdf_page_index]
        
        # Render page to image at specified DPI
        pix = page.get_pixmap(dpi=dpi)
        img = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.height, pix.width, pix.n)
        
        # Convert to RGB if needed
        if pix.n == 4:  # RGBA
            img = cv2.cvtColor(img, cv2.COLOR_RGBA2RGB)
        elif pix.n == 1:  # Grayscale
            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)
        
        # Convert to PIL image for Tesseract
        pil_img = Image.fromarray(img)
        
        # Perform OCR
        text = pytesseract.image_to_string(pil_img)
        
        # For OCR case, we only create one content block with all the text
        # regardless of paragraphs or line breaks
        if text and text.strip():
            result["extraction"]["content_blocks"] = [{
                "block_id": 1,
                "type": "text",
                "content": text.strip()
            }]
        else:
            result["extraction"]["content_blocks"] = [{
                "block_id": 1,
                "type": "text",
                "content": "No text content could be extracted via OCR from this page."
            }]
                
    except Exception as e:
        # Handle extraction errors
        result["extraction"]["content_blocks"] = [{
            "block_id": 1,
            "type": "text",
            "content": f"Error during OCR extraction: {str(e)}"
        }]
    
    # Calculate and record processing time
    processing_time = time.time() - start_time
    result["extraction"]["processing_time"] = f"{processing_time:.2f} seconds"
    
    return result

def process_pdf_pages(pdf_path, analysis_json_path, output_json_path, dpi=300):
    """
    Process all PDF pages that need OCR extraction based on analysis results
    
    Args:
        pdf_path (str): Path to the PDF file
        analysis_json_path (str): Path to the analysis JSON file
        output_json_path (str): Path to save the extraction results
        dpi (int): DPI resolution for rendering PDF to image
    """
    # Load analysis results
    with open(analysis_json_path, 'r', encoding='utf-8') as f:
        analysis_data = json.load(f)
    
    # Create or load output JSON
    if os.path.exists(output_json_path):
        with open(output_json_path, 'r', encoding='utf-8') as f:
            output_data = json.load(f)
    else:
        # Get PDF metadata
        with open(pdf_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            total_pages = len(pdf_reader.pages)
        
        # Create new output structure
        output_data = {
            "metadata": {
                "filename": Path(pdf_path).name,
                "total_pages": total_pages,
                "extraction_date": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "processing_time": "0 seconds"  # Will be updated later
            },
            "pages": {}
        }
        
        # Initialize pages structure with analysis data
        for page_num, page_analysis in analysis_data.items():
            output_data["pages"][page_num] = {
                "analysis": page_analysis
            }
    
    start_time = time.time()
    processed_count = 0
    
    # Process pages that need OCR extraction
    for page_num, page_data in analysis_data.items():
        # Check if this page needs OCR extraction
        if (page_data.get("ocr_status", False) and 
            not page_data.get("line_status", True) and 
            not page_data.get("ai_status", True)):
            
            # Check if this page has already been processed with OCR extraction
            if (page_num in output_data["pages"] and 
                "extraction" in output_data["pages"][page_num] and 
                output_data["pages"][page_num]["extraction"]["method"] == "ocr"):
                print(f"Page {page_num} already processed with OCR extraction. Skipping.")
                continue
            
            print(f"Processing page {page_num} with OCR extraction...")
            
            # Get existing result if available, otherwise create new
            existing_result = output_data["pages"].get(page_num, {"analysis": page_data})
            
            # Extract content
            result = extract_with_ocr_method(pdf_path, int(page_num), existing_result, dpi)
            
            # Update output data
            output_data["pages"][page_num] = result
            processed_count += 1
    
    # Update metadata
    total_processing_time = time.time() - start_time
    output_data["metadata"]["processing_time"] = f"{total_processing_time:.2f} seconds"
    
    # Save output data
    with open(output_json_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=4, ensure_ascii=False)
    
    print(f"OCR extraction completed. Processed {processed_count} pages.")
    return output_data

if __name__ == "__main__":
    # Example usage
    pdf_path = "ABF Indonesia Bond Index Fund.pdf"  # Replace with your PDF path
    analysis_json_path = "sample.json"  # Path to analysis JSON
    output_json_path = "hasil_ekstraksi.json"  # Path to save extraction results
    
    process_pdf_pages(pdf_path, analysis_json_path, output_json_path)



aturan 3 akan menggunakan script pdf_only.py sebagai berikut

"""
direct_extractor.py - Module for direct PDF text extraction
For pages where OCR isn't needed and no special formatting is detected
"""

import os
import json
import PyPDF2
import datetime
import time
from pathlib import Path

def extract_with_direct_method(pdf_path, page_num, existing_result=None):
    """
    Extract text directly from PDF using PyPDF2 for pages that don't need 
    special processing.
    
    Args:
        pdf_path (str): Path to the PDF file
        page_num (int): Page number to extract (1-based indexing)
        existing_result (dict, optional): Existing extraction result to update
        
    Returns:
        dict: The extraction result for the specified page
    """
    start_time = time.time()
    
    # Create result structure if not provided
    if existing_result is None:
        result = {
            "analysis": {
                "ocr_status": False,
                "line_status": False,
                "ai_status": False
            },
            "extraction": {
                "method": "direct_extraction",
                "processing_time": None,
                "content_blocks": []
            }
        }
    else:
        result = existing_result
        # Set extraction method and initialize content blocks
        result["extraction"] = {
            "method": "direct_extraction",
            "processing_time": None,
            "content_blocks": []
        }
    
    try:
        # Open PDF and extract text from the specific page
        with open(pdf_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            
            # Adjust for 0-based indexing in PyPDF2
            pdf_page_index = page_num - 1
            
            if pdf_page_index < 0 or pdf_page_index >= len(pdf_reader.pages):
                raise ValueError(f"Page {page_num} does not exist in PDF with {len(pdf_reader.pages)} pages")
            
            pdf_page = pdf_reader.pages[pdf_page_index]
            text = pdf_page.extract_text()
            
            # Create a single content block for the extracted text, regardless of paragraphs
            if text and text.strip():
                result["extraction"]["content_blocks"] = [{
                    "block_id": 1,
                    "type": "text",
                    "content": text.strip()
                }]
            else:
                result["extraction"]["content_blocks"] = [{
                    "block_id": 1,
                    "type": "text",
                    "content": "No text content could be extracted directly from this page."
                }]
                
    except Exception as e:
        # Handle extraction errors
        result["extraction"]["content_blocks"] = [{
            "block_id": 1,
            "type": "text",
            "content": f"Error during direct extraction: {str(e)}"
        }]
    
    # Calculate and record processing time
    processing_time = time.time() - start_time
    result["extraction"]["processing_time"] = f"{processing_time:.2f} seconds"
    
    return result

def process_pdf_pages(pdf_path, analysis_json_path, output_json_path):
    """
    Process all PDF pages that need direct extraction based on analysis results
    
    Args:
        pdf_path (str): Path to the PDF file
        analysis_json_path (str): Path to the analysis JSON file
        output_json_path (str): Path to save the extraction results
    """
    # Load analysis results
    with open(analysis_json_path, 'r', encoding='utf-8') as f:
        analysis_data = json.load(f)
    
    # Create or load output JSON
    if os.path.exists(output_json_path):
        with open(output_json_path, 'r', encoding='utf-8') as f:
            output_data = json.load(f)
    else:
        # Get PDF metadata
        with open(pdf_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            total_pages = len(pdf_reader.pages)
        
        # Create new output structure
        output_data = {
            "metadata": {
                "filename": Path(pdf_path).name,
                "total_pages": total_pages,
                "extraction_date": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "processing_time": "0 seconds"  # Will be updated later
            },
            "pages": {}
        }
        
        # Initialize pages structure with analysis data
        for page_num, page_analysis in analysis_data.items():
            output_data["pages"][page_num] = {
                "analysis": page_analysis
            }
    
    start_time = time.time()
    processed_count = 0
    
    # Process pages that need direct extraction
    for page_num, page_data in analysis_data.items():
        # Check if this page needs direct extraction
        if (not page_data.get("ocr_status", True) and 
            not page_data.get("line_status", True) and 
            not page_data.get("ai_status", True)):
            
            # Check if this page has already been processed with direct extraction
            if (page_num in output_data["pages"] and 
                "extraction" in output_data["pages"][page_num] and 
                output_data["pages"][page_num]["extraction"]["method"] == "direct_extraction"):
                print(f"Page {page_num} already processed with direct extraction. Skipping.")
                continue
            
            print(f"Processing page {page_num} with direct extraction...")
            
            # Get existing result if available, otherwise create new
            existing_result = output_data["pages"].get(page_num, {"analysis": page_data})
            
            # Extract content
            result = extract_with_direct_method(pdf_path, int(page_num), existing_result)
            
            # Update output data
            output_data["pages"][page_num] = result
            processed_count += 1
    
    # Update metadata
    total_processing_time = time.time() - start_time
    output_data["metadata"]["processing_time"] = f"{total_processing_time:.2f} seconds"
    
    # Save output data
    with open(output_json_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=4, ensure_ascii=False)
    
    print(f"Direct extraction completed. Processed {processed_count} pages.")
    return output_data

if __name__ == "__main__":
    # Example usage
    pdf_path = "ABF Indonesia Bond Index Fund.pdf"  # Replace with your PDF path
    analysis_json_path = "sample.json"  # Path to analysis JSON
    output_json_path = "hasil_ekstraksi.json"  # Path to save extraction results
    
    process_pdf_pages(pdf_path, analysis_json_path, output_json_path)


kedua sript tersebut akan menuliskan hasil ekstraksinya di file json Bernama hasil_ekstraksi.json dengan guide penyimpanan seperti ini

{
    "metadata": {
        "filename":"file_name",
        "total_pages": "value-total_pages",
        "extraction_date": "value-extraction_date",
        "processing_time": "value-processing_time"
    },
    "pages": {
        "1": {
            "analysis":{
                "ocr_status": false,
                "line_status": false,
                "ai_status": false
            },
            "extraction": {
                "method": "direct_extraction",
                "processing_time": "value-processing_time",
                "content_blocks":[
                    {
                        "block_id": 1,
                        "type": "text",
                        "content": "value-content"
                    }
                ]
            }
        },
        "2": {
            "analysis":{
                "ocr_status": true,
                "line_status": false,
                "ai_status": false
            },
            "extraction": {
                "method": "ocr",
                "processing_time": "value-processing_time",
                "content_blocks":[
                    {
                        "block_id": 1,
                        "type": "text",
                        "content": "value-content"
                    }
                ]
            }
        },
        "3": {
            "analysis":{
                "ocr_status": false,
                "line_status": true,
                "ai_status": true
            },
            "extraction": {
                "method": "multimodal_llm",
                "processing_time": "value-processing_time",
                "content_blocks":[
                    {
                        "block_id": 1,
                        "type": "text",
                        "content": "value-content"
                    },
                    {
                        "block_id": 2,
                        "type": "table",
                        "title": "value-title",
                        "data": [
                            {"header_1": "value-header_1-row_1", "header_2": "value-header_2-row_1"},
                            {"header_1": "value-header_1-row_2", "header_2": "value-header_2-row_2"},
                            {"header_1": "value-header_1-row_3", "header_2": "value-header_2-row_3"},
                            {"header_1": "value-header_1-row_4", "header_2": "value-header_2-row_4"},
                            {"header_1": "value-header_1-row_5", "header_2": "value-header_2-row_5"}
                        ],
                        "summary_table": "value-summary_table"
                    },
                    {
                        "block_id": 3,
                        "type": "text",
                        "content": "value-content"
                    },
                    {
                        "block_id": 4,
                        "type": "chart",
                        "chart_type": "line",
                        "title": "value-title",
                        "data": {
                            "labels": ["labels_1","labels_2","labels_3","labels_4","labels_5"],
                            "datasets": [
                                {
                                    "label": "label_1",
                                    "values": [6.8, 7.2, 5.9, 6.7, 7.5]
                                },
                                {
                                    "label": "label_2",
                                    "values": [6.2, 6.5, 5.4, 6.1, 6.9]
                                }
                            ]
                        },
                        "summary_chart": "value-summary_chart"
                    },
                    {
                        "block_id": 5,
                        "type": "text",
                        "content": "value-content"
                    },
                    {
                        "block_id": 6,
                        "type": "flowchart",
                        "title": "value-title",
                        "elements": [
                            {"type": "node", "id": "1", "text": "value-node_1", "connects_to": ["2"]},
                            {"type": "node", "id": "2", "text": "value-node_2", "connects_to": ["3"]},
                            {"type": "node", "id": "3", "text": "value-node_3", "connects_to": ["4"]},
                            {"type": "node", "id": "4", "text": "value-node_4", "connects_to": ["5"]},
                            {"type": "node", "id": "5", "text": "value-node_5", "connects_to": ["6"]},
                            {"type": "node", "id": "6", "text": "value-node_6", "connects_to": ["1"]}
                        ],
                        "summary_flowchart": "value-summary_flowchart"
                    },
                    {
                        "block_id": 7,
                        "type": "text",
                        "content": "value-content"
                    },
                    {
                        "block_id": 8,
                        "type": "image",
                        "description_image": "value-description_image"
                    }
                ]
            }
        },
        "4": {
            "analysis":{
                "ocr_status": false,
                "line_status": true,
                "ai_status": true
            },
            "extraction": {
                "method": "multimodal_llm",
                "processing_time": "value-processing_time",
                "content_blocks":[
                    {
                        "block_id": 1,
                        "type": "text",
                        "content": "value-content"
                    },
                    {
                        "block_id": 2,
                        "type": "table",
                        "title": "value-title",
                        "data": [
                            {"header_1": "value-header_1-row_1", "header_2": "value-header_2-row_1"},
                            {"header_1": "value-header_1-row_2", "header_2": "value-header_2-row_2"},
                            {"header_1": "value-header_1-row_3", "header_2": "value-header_2-row_3"},
                            {"header_1": "value-header_1-row_4", "header_2": "value-header_2-row_4"},
                            {"header_1": "value-header_1-row_5", "header_2": "value-header_2-row_5"}
                        ],
                        "summary_table": "value-summary_table"
                    },
                    {
                        "block_id": 3,
                        "type": "text",
                        "content": "value-content"
                    },
                    {
                        "block_id": 4,
                        "type": "chart",
                        "chart_type": "line",
                        "title": "value-title",
                        "data": {
                            "labels": ["labels_1","labels_2","labels_3","labels_4","labels_5"],
                            "datasets": [
                                {
                                    "label": "label_1",
                                    "values": [6.8, 7.2, 5.9, 6.7, 7.5]
                                },
                                {
                                    "label": "label_2",
                                    "values": [6.2, 6.5, 5.4, 6.1, 6.9]
                                }
                            ]
                        },
                        "summary_chart": "value-summary_chart"
                    },
                    {
                        "block_id": 5,
                        "type": "text",
                        "content": "value-content"
                    },
                    {
                        "block_id": 6,
                        "type": "flowchart",
                        "title": "value-title",
                        "elements": [
                            {"type": "node", "id": "1", "text": "value-node_1", "connects_to": ["2"]},
                            {"type": "node", "id": "2", "text": "value-node_2", "connects_to": ["3"]},
                            {"type": "node", "id": "3", "text": "value-node_3", "connects_to": ["4"]},
                            {"type": "node", "id": "4", "text": "value-node_4", "connects_to": ["5"]},
                            {"type": "node", "id": "5", "text": "value-node_5", "connects_to": ["6"]},
                            {"type": "node", "id": "6", "text": "value-node_6", "connects_to": ["1"]}
                        ],
                        "summary_flowchart": "value-summary_flowchart"
                    },
                    {
                        "block_id": 7,
                        "type": "text",
                        "content": "value-content"
                    },
                    {
                        "block_id": 8,
                        "type": "image",
                        "description_image": "value-description_image"
                    }
                ]
            }
        }
    }
}

sekarang saya ingin meminta bantuan mu
karena saya telah membuat script :
1. identifier.py
2. ocr_only.py
3. pdf_only.py

apakah kamu bisa membuat untuk ekstraksi dengan penanganan multimodal apinya? jika bisa saya telah memiliki script yang saya harap bisa jadi guide mu dalam membuat multimodal api ekstraktornya dengan nama multimodal_only.py


import PIL.Image
import google.generativeai as genai
from google.generativeai import types
import os
from dotenv import load_dotenv

load_dotenv()

GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')

image_path_1 = "image/7_Tabel_N_Halaman_Normal_V1_page-0001.jpg"
image_path_2 = "image/7_Tabel_N_Halaman_Normal_V1_page-0002.jpg"

pil_image_1 = PIL.Image.open(image_path_1)
pil_image_2 = PIL.Image.open(image_path_2)

genai.configure(api_key=GOOGLE_API_KEY) #configure api key

model = genai.GenerativeModel('gemini-2.0-flash') #gunakan model yang sesuai

response = model.generate_content(
    [
        "apakah kamu bisa melakukan ekstraksi tabel dari file pdf yang saya berikan ini? serta outputnya kalau bisa dalam bentuk json",
        pil_image_1, pil_image_2
    ]
)

print(response.text)


kemudian karena image harus disimpan sementara atau dirender sebelum dikirimkan ke llm multimodal api
bagaimana kalau kita spesifikkan penyimpanan sementara akan ada didalam folder temporary_dir dengan struktur penyimpanan script seperti ini

.
├── identifier.py
├── ocr_only.py
├── pdf_only.py
├── multimodal_only.py
├── temporary_dir/
│   ├── image_page_1.png
│   └── image_page_2.png
├── ABF Indonesia Bond Index Fund.pdf
├── hasil_ekstraksi.json
└── sample.json

coba pahami terlebih dahulu perintah atau permintaan saya sebelum membuat multimodal_only.py
jika ada yang perlu diklarifikasikan jangan ragu tanyakan ke saya terlebih dahulu